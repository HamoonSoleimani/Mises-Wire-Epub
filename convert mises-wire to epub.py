{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "A versatile Python script to convert Mises Wire articles into a single EPUB file.\n",
    "It fetches articles from paginated index pages (e.g., ?page=1, ?page=2, … up to ?page=1000),\n",
    "extracts each article’s content, and combines them as chapters in one EPUB file with a proper\n",
    "table of contents and indexing.\n",
    "\n",
    "Requirements:\n",
    "    - requests\n",
    "    - beautifulsoup4\n",
    "    - readability-lxml\n",
    "    - ebooklib\n",
    "\n",
    "Usage:\n",
    "    python convert_mises_wire.py --all [--pages <number_of_pages>] [--save_dir <directory>]\n",
    "    --pages defaults to 1000.\n",
    "    --save_dir is the directory where the combined EPUB will be saved.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "import argparse\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from readability import Document\n",
    "from ebooklib import epub\n",
    "import traceback\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "HEADERS = {\n",
    "    'User-Agent': ('Mozilla/5.0 (Windows NT 10.0; Win64; x64) '\n",
    "                   'AppleWebKit/537.36 (KHTML, like Gecko) '\n",
    "                   'Chrome/91.0.4472.124 Safari/537.36')\n",
    "}\n",
    "\n",
    "def sanitize_filename(title):\n",
    "    filename = title.replace(\" \", \"_\")\n",
    "    filename = re.sub(r'[^\\w\\s.-]', '', filename)\n",
    "    filename = filename.strip('_').strip()\n",
    "    return filename[:200]\n",
    "\n",
    "def get_article_links(index_url, max_pages=1000):\n",
    "    \"\"\"\n",
    "    Fetch article URLs from the Mises Wire index and paginated pages.\n",
    "    Iterates through pages 1 to max_pages.\n",
    "    Extracts article links from <article> elements or falls back to scanning <a> tags.\n",
    "    Skips RSS feed links.\n",
    "    Returns a list of absolute URLs.\n",
    "    \"\"\"\n",
    "    all_article_links = set()\n",
    "    for page_num in range(1, max_pages + 1):\n",
    "        page_url = f\"{index_url}?page={page_num}\" if page_num > 1 else index_url\n",
    "        logging.debug(f\"Fetching index page: {page_url}\")\n",
    "        try:\n",
    "            response = requests.get(page_url, headers=HEADERS)\n",
    "            response.raise_for_status()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logging.error(f\"Failed to fetch index page {page_url}: {e}\")\n",
    "            logging.debug(traceback.format_exc())\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        articles = soup.find_all('article')\n",
    "        if articles:\n",
    "            for art in articles:\n",
    "                a_tag = art.find('a', href=True)\n",
    "                if a_tag:\n",
    "                    href = a_tag['href']\n",
    "                    if 'rss.xml' in href:\n",
    "                        continue\n",
    "                    absolute_url = urljoin(index_url, href)\n",
    "                    all_article_links.add(absolute_url)\n",
    "        else:\n",
    "            # Fallback: scan all <a> tags containing '/wire/' and not RSS.\n",
    "            for a in soup.find_all('a', href=True):\n",
    "                href = a['href']\n",
    "                if '/wire/' in href and 'rss.xml' not in href:\n",
    "                    absolute_url = urljoin(index_url, href)\n",
    "                    all_article_links.add(absolute_url)\n",
    "        logging.debug(f\"Accumulated {len(all_article_links)} unique links so far after page {page_num}.\")\n",
    "    article_links_list = list(all_article_links)\n",
    "    logging.info(f\"Total article links found across {max_pages} pages: {len(article_links_list)}\")\n",
    "    return article_links_list\n",
    "\n",
    "def process_article(url):\n",
    "    \"\"\"\n",
    "    Fetch an article from the URL, extract its content using readability\n",
    "    (with a manual fallback), and return a tuple of (title, chapter) where chapter is an EpubHtml item.\n",
    "    Returns (None, None) if processing fails.\n",
    "    \"\"\"\n",
    "    logging.debug(f\"Processing URL: {url}\")\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Failed to fetch {url}: {e}\")\n",
    "        logging.debug(traceback.format_exc())\n",
    "        return None, None\n",
    "\n",
    "    try:\n",
    "        doc = Document(response.text)\n",
    "        title = doc.short_title() or \"untitled\"\n",
    "        cleaned_html = doc.summary()\n",
    "        logging.debug(f\"Extracted title via readability: {title}\")\n",
    "        if not title or not cleaned_html:\n",
    "            logging.error(f\"Readability extraction failed for {url}; using manual fallback.\")\n",
    "            title, cleaned_html = manual_extraction_fallback(response.text)\n",
    "            if not title or not cleaned_html:\n",
    "                logging.error(f\"Manual extraction also failed for {url}.\")\n",
    "                return None, None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during readability processing for {url}: {e}\")\n",
    "        logging.debug(traceback.format_exc())\n",
    "        title, cleaned_html = manual_extraction_fallback(response.text)\n",
    "        if not title or not cleaned_html:\n",
    "            logging.error(f\"Manual fallback extraction failed for {url}.\")\n",
    "            return None, None\n",
    "\n",
    "    chapter_filename = sanitize_filename(title) + '.xhtml'\n",
    "    chapter = epub.EpubHtml(title=title, file_name=chapter_filename, lang='en')\n",
    "    chapter.content = cleaned_html.encode('utf-8')\n",
    "    return title, chapter\n",
    "\n",
    "def manual_extraction_fallback(html_content):\n",
    "    \"\"\"\n",
    "    A fallback extraction method if readability fails.\n",
    "    Searches for common selectors to extract the title and content.\n",
    "    \"\"\"\n",
    "    logging.debug(\"Attempting manual extraction fallback.\")\n",
    "    try:\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        title_element = (soup.find('h1', class_='page-header__title') or\n",
    "                         soup.find('h1', class_='entry-title') or\n",
    "                         soup.find('h1', itemprop='headline'))\n",
    "        if not title_element:\n",
    "            logging.warning(\"Manual extraction: Title element not found.\")\n",
    "            return None, None\n",
    "        title = title_element.get_text(strip=True)\n",
    "        logging.debug(f\"Manual extraction: Found title: {title}\")\n",
    "        content_element = soup.find('div', class_='post-entry')\n",
    "        if not content_element:\n",
    "            logging.warning(\"Manual extraction: Content container not found; using entire body.\")\n",
    "            content = str(soup.body)\n",
    "        else:\n",
    "            article_paragraphs = content_element.find_all('p')\n",
    "            if article_paragraphs:\n",
    "                content = \"\\n\\n\".join(p.get_text(strip=True) for p in article_paragraphs)\n",
    "            else:\n",
    "                content = content_element.get_text(separator='\\n', strip=True)\n",
    "        cleaned_html_fallback = f\"<h1>{title}</h1><article>{content.replace('\\n\\n', '<p></p>')}</article>\"\n",
    "        return title, cleaned_html_fallback\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Manual extraction fallback failed: {e}\")\n",
    "        logging.debug(traceback.format_exc())\n",
    "        return None, None\n",
    "\n",
    "def create_combined_epub(chapters, save_dir, epub_title=\"Mises Wire Collection\"):\n",
    "    \"\"\"\n",
    "    Create a single EPUB file from a list of chapters.\n",
    "    Each chapter is an EpubHtml item. The EPUB includes a table of contents and proper spine ordering.\n",
    "    \"\"\"\n",
    "    book = epub.EpubBook()\n",
    "    book.set_title(epub_title)\n",
    "    book.add_author(\"Mises Wire\")\n",
    "    book.set_language('en')\n",
    "\n",
    "    # Add each chapter to the book.\n",
    "    for chapter in chapters:\n",
    "        book.add_item(chapter)\n",
    "\n",
    "    # Build TOC and spine list.\n",
    "    toc = []\n",
    "    spine = ['nav']\n",
    "    for chapter in chapters:\n",
    "        toc.append(epub.Link(chapter.file_name, chapter.title, chapter.file_name))\n",
    "        spine.append(chapter)\n",
    "\n",
    "    book.toc = tuple(toc)\n",
    "    book.spine = spine\n",
    "\n",
    "    # Add navigation files.\n",
    "    book.add_item(epub.EpubNcx())\n",
    "    book.add_item(epub.EpubNav())\n",
    "\n",
    "    # Add basic style.\n",
    "    style = 'BODY { font-family: Times, serif; }'\n",
    "    nav_css = epub.EpubItem(uid=\"style_nav\", file_name=\"style/nav.css\",\n",
    "                            media_type=\"text/css\", content=style.encode('utf-8'))\n",
    "    book.add_item(nav_css)\n",
    "\n",
    "    safe_title = sanitize_filename(epub_title)\n",
    "    filename = os.path.join(save_dir, safe_title + '.epub')\n",
    "    try:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        epub.write_epub(filename, book, {})\n",
    "        logging.info(f\"Saved combined EPUB: {filename}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to write combined EPUB: {e}\")\n",
    "        logging.debug(traceback.format_exc())\n",
    "\n",
    "def main():\n",
    "    logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='Convert Mises Wire articles into a single EPUB file.')\n",
    "    parser.add_argument('--all', action='store_true', help='Convert all articles from Mises Wire index pages.')\n",
    "    parser.add_argument('--pages', type=int, default=1000, help='Number of index pages to check when using --all.')\n",
    "    parser.add_argument('--save_dir', type=str, default=r\"/path/to/folder/, help='Directory to save the combined EPUB file.')\n",
    "    parser.add_argument('--epub_title', type=str, default=\"Mises Wire Collection\", help='Title for the combined EPUB file.')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.all:\n",
    "        index_url = \"https://mises.org/wire\"\n",
    "        article_links = get_article_links(index_url, max_pages=args.pages)\n",
    "        logging.info(f\"Found {len(article_links)} article links; starting processing for combined EPUB.\")\n",
    "        chapters = []\n",
    "        for url in article_links:\n",
    "            title, chapter = process_article(url)\n",
    "            if title and chapter:\n",
    "                logging.info(f\"Processed article: {title}\")\n",
    "                chapters.append(chapter)\n",
    "            else:\n",
    "                logging.error(f\"Skipping article at {url} due to processing errors.\")\n",
    "        if chapters:\n",
    "            create_combined_epub(chapters, args.save_dir, epub_title=args.epub_title)\n",
    "        else:\n",
    "            logging.error(\"No chapters were successfully processed. Combined EPUB not created.\")\n",
    "    else:\n",
    "        logging.error(\"No mode specified. Use --all to process all articles into one EPUB.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
